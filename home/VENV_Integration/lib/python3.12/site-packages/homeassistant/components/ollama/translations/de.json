{
    "config": {
        "error": {
            "cannot_connect": "Verbindung fehlgeschlagen",
            "download_failed": "Herunterladen des Modells fehlgeschlagen",
            "unknown": "Unerwarteter Fehler"
        },
        "progress": {
            "download": "Bitte warte, w\u00e4hrend das Modell heruntergeladen wird. Dies kann sehr lange dauern. Weitere Informationen findest du in den Protokollen deines Ollama-Servers."
        },
        "step": {
            "download": {
                "title": "Modell wird heruntergeladen"
            },
            "user": {
                "data": {
                    "model": "Modell",
                    "url": "URL"
                }
            }
        }
    },
    "options": {
        "step": {
            "init": {
                "data": {
                    "keep_alive": "Am Leben erhalten",
                    "llm_hass_api": "Home Assistant steuern",
                    "max_history": "Max. Verlaufsnachrichten",
                    "num_ctx": "Gr\u00f6\u00dfe des Kontextfensters",
                    "prompt": "Anweisungen"
                },
                "data_description": {
                    "keep_alive": "Dauer in Sekunden, die Ollama ben\u00f6tigt, um das Modell im Speicher zu behalten. -1 = unbegrenzt, 0 = nie.",
                    "num_ctx": "Maximale Anzahl von Texttoken, die das Modell verarbeiten kann. Verringere den Wert, um den Ollama-RAM zu reduzieren, oder erh\u00f6he ihn bei einer gro\u00dfen Anzahl verf\u00fcgbarer Entit\u00e4ten.",
                    "prompt": "Gib Anweisungen, wie der LLM reagieren soll. Dies kann eine Vorlage sein."
                }
            }
        }
    }
}