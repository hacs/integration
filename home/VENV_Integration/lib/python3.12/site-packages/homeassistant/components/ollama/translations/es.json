{
    "config": {
        "error": {
            "cannot_connect": "No se pudo conectar",
            "download_failed": "Fallo en la descarga del modelo",
            "unknown": "Error inesperado"
        },
        "progress": {
            "download": "Por favor, espera mientras se descarga el modelo, lo que puede llevar mucho tiempo. Revisa los registros de tu servidor Ollama para obtener m\u00e1s detalles."
        },
        "step": {
            "download": {
                "title": "Descargando modelo"
            },
            "user": {
                "data": {
                    "model": "Modelo",
                    "url": "URL"
                }
            }
        }
    },
    "options": {
        "step": {
            "init": {
                "data": {
                    "keep_alive": "Mantener vivo",
                    "llm_hass_api": "Controla Home Assistant",
                    "max_history": "M\u00e1ximo de mensajes del historial",
                    "num_ctx": "Tama\u00f1o de la ventana de contexto",
                    "prompt": "Instrucciones"
                },
                "data_description": {
                    "keep_alive": "Duraci\u00f3n en segundos que tarda Ollama en mantener el modelo en la memoria. -1 = indefinido, 0 = nunca.",
                    "num_ctx": "N\u00famero m\u00e1ximo de tokens de texto que el modelo puede procesar. Disminuye el valor para reducir la RAM de Ollama o aum\u00e9ntalo para una gran cantidad de entidades expuestas.",
                    "prompt": "Indica c\u00f3mo debe responder el LLM. Puede ser una plantilla."
                }
            }
        }
    }
}