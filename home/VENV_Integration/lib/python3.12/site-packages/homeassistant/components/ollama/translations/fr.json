{
    "config": {
        "error": {
            "cannot_connect": "\u00c9chec de connexion",
            "download_failed": "Le t\u00e9l\u00e9chargement du mod\u00e8le a \u00e9chou\u00e9",
            "unknown": "Erreur inattendue"
        },
        "progress": {
            "download": "Veuillez patienter pendant le t\u00e9l\u00e9chargement du mod\u00e8le, ce qui peut prendre beaucoup de temps. V\u00e9rifiez les journaux de votre serveur Ollama pour plus de d\u00e9tails."
        },
        "step": {
            "download": {
                "title": "T\u00e9l\u00e9chargement du mod\u00e8le"
            },
            "user": {
                "data": {
                    "model": "Mod\u00e8le",
                    "url": "URL"
                }
            }
        }
    },
    "options": {
        "step": {
            "init": {
                "data": {
                    "keep_alive": "Maintenir en vie",
                    "llm_hass_api": "Contr\u00f4ler Home Assistant",
                    "max_history": "Nombre maximal de messages d'historique",
                    "num_ctx": "Taille de la fen\u00eatre contextuelle",
                    "prompt": "Instructions"
                },
                "data_description": {
                    "keep_alive": "Dur\u00e9e en secondes pendant laquelle Ollama garde le mod\u00e8le en m\u00e9moire. -1 = ind\u00e9fini, 0 = jamais.",
                    "num_ctx": "Nombre maximal de jetons de texte que le mod\u00e8le peut traiter. R\u00e9duisez-le pour r\u00e9duire la RAM Ollama ou augmentez-le pour un grand nombre d'entit\u00e9s expos\u00e9es.",
                    "prompt": "Expliquez comment le LLM doit r\u00e9agir. Cela peut \u00eatre un mod\u00e8le."
                }
            }
        }
    }
}